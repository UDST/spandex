import ConfigParser
import logging
import os
import subprocess

import pandas as pd
import psycopg2


# Set up logging system.
logging.basicConfig()
logger = logging.getLogger(__name__)


def load_config(config_dir='config'):
    """Returns a ConfigParser object.

    Configuration is loaded from defaults.cfg and user.cfg in config_dir.

    """
    # Load configuration using ConfigParser.
    logger.debug("Loading configuration from %s" % config_dir)
    config = ConfigParser.RawConfigParser()
    try:
        config.read([os.path.join(config_dir, 'defaults.cfg'),
                     os.path.join(config_dir, 'user.cfg')])
        return config
    except ConfigParser.ParsingError:
        logger.exception("Error parsing configuration")
        raise


def logf(level, f):
    """Log each line of a file-like object at the specified severity level."""
    for line in f:
        if line:
            logger.log(level, line.strip())


class DataLoader(object):
    """Data loader class with support for importing shapefiles.

    Some example usage:

        loader = DataLoader()

        # Load data.
        loader.load_shp('parcel/Alameda.shp', 'staging.alameda')

        # Run SQL command.
        with loader.connection.cursor() as cur:
            cur.execute("SELECT DISTINCT luc_desc FROM staging.alameda;")
            rows = cur.fetchall()

        # Commit and close connection.
        loader.close()

    If used in a with statement, the close method is automatically called at
    the end of the with block.

    Methods:
        close:      Commit and close PostgreSQL connection.
        load_shp:   Load a shapefile from the directory into a PostGIS table.

    Attributes:
        connection: psycopg2 connection object.
        directory:  Path to the directory containing the shapefiles.
        srid:       Spatial Reference System Identifier (SRID).

    Constructor arguments:
        config_dir: Path to configuration directory containing default.cfg
                    and/or user.cfg. If None, attributes must be passed as
                    additional constructor arguments. Otherwise, passed
                    attributes override configuration.

    """

    def __init__(self, config_dir='config', connection=None, directory=None,
                 srid=None):
        # If configuration directory is defined, load it.
        if config_dir:
            config = load_config(config_dir)
            db_config = dict(config.items('database'))

        # If constructor arguments are not populated, use configuration.
        if not connection:
            connection = psycopg2.connect(**db_config)
        if not directory:
            directory = config.get('data', 'directory')
        if not srid:
            srid = config.get('data', 'srid')

        # Assign arguments to class attributes.
        self.connection = connection
        if os.path.exists(directory):
            self.directory = directory
        else:
            raise IOError("Directory does not exist: %s" % directory)
        self.srid = int(srid)

    def __enter__(self, *args, **kwargs):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_value:
            logger.error("Transaction rollback",
                         exc_info=(exc_type, exc_value, traceback))
            self.connection.rollback()
        else:
            self.connection.commit()
        self.connection.close()

    def close(self):
        self.connection.commit()
        return self.connection.close()

    def load_shp(self, filename, table, drop=False, append=False):
        """Load a shapefile from the directory into a PostGIS table.

        This is a Python wrapper for shp2gpsql. shp2pgsql is spawned by
        subprocess. Commands generated by shp2pgsql are executed on a
        psycopg2 cursor object. For performance, the PostgreSQL "dump"
        format is used instead of the default "insert" SQL format.

        Args:
            filename: Shapefile, relative to the data directory.
            table:    PostGIS table name (optionally schema-qualified).
            drop:     Whether to drop a table that already exists.
                      Defaults to False.
            append:   Whether to append to an existing table, instead of
                      creating one. Defaults to False.
        """
        logger.info("Loading table %s from file %s." % (table, filename))
        filepath = os.path.join(self.directory, filename)

        try:
            with self.connection.cursor() as cur:

                if drop:
                    # Drop the existing table.
                    cur.execute('DROP TABLE IF EXISTS %s' % table)

                if not append:
                    # Create the new table itself without adding actual data.
                    create_table = subprocess.Popen(['shp2pgsql', '-p', '-I',
                                                     '-s', str(self.srid),
                                                     filepath, table],
                                                     stdout=subprocess.PIPE,
                                                     stderr=subprocess.PIPE)
                    try:
                        command = ''
                        for line in create_table.stdout:
                            if line and not (line.startswith('BEGIN') or
                                             line.startswith('COMMIT')):
                                command += line
                        cur.execute(command)
                    finally:
                        logf(logging.DEBUG, create_table.stderr)
                    create_table.wait()

                # Append data to existing or newly-created table.
                append_data = subprocess.Popen(['shp2pgsql', '-a', '-D', '-I',
                                                '-s', str(self.srid),
                                                filepath, table],
                                                stdout=subprocess.PIPE,
                                                stderr=subprocess.PIPE)
                try:
                    while True:
                        line = append_data.stdout.readline()
                        if line.startswith('COPY'):
                            break
                    cur.copy_expert(line, append_data.stdout)
                finally:
                    logf(logging.DEBUG, append_data.stderr)
                append_data.wait()

            # Commit when finished.
            self.connection.commit()

        except:
            logger.exception("Transaction rollback")
            self.connection.rollback()
            raise


def load_shapefile(shp_path, db_table_name, config_dir, srid=None):
    """
    Load single shapefile to PostGIS using DataLoader.

    Parameters
    ----------
    shp_path : str
        Path to shapefile.
    db_table_name : str
        Name of resulting PostGIS database table.
    config_dir : str
        Path to spandex configuration directory
    srid : int, optional
        SRID of shapefile, if different from SRID specified in spandex
        configuration file.

    Returns
    -------
    None : None
        Loads shapefile to the database (returns nothing)

    """
    with DataLoader(config_dir=config_dir) as loader:
        if srid:  loader.srid = srid
        loader.load_shp(shp_path, db_table_name, drop=True)


def load_multiple_shp(shapefiles, data_dir, config_dir):
    """
    Load multiple shapefiles to PostGIS according to a given dictionary
    of shapefile information.

    Parameters
    ----------
    shapefiles : dict
        Dictionary of dictionaries where the top-level key is shapefile category,
        which also corresponds to the name of the directory within the data_dir
        containing this category of shapefiles. The sub-dictionaries are
        dictionaries where the keys correspond to database table name and the
        value is a tuple of the form (shapefile_file_name, SRID).  If SRID is
        None, then default config SRID is used.

        Example dictionary
             {'parcels' :  ##Looks for 'parcels' directory within the data_dir
                  {'marin':('Marin_2006_CWP.shp', 2872),  ##Looks for 'marin' directory within parcels dir
                  'napa':('Napa_Parcels.shp', 2226),
                  },
              'boundaries' :
                  {'blocks':('block10_gba.shp', 26910),
                   'block_groups':('blockgroup10_gba.shp',26910),
                  },
             }
    data_dir : str
        Path to the input data directory.  This base directory should contain
        subdirectories corresponding to each shapefile category, which in turn
        should contain a subdirectory for each shapefile.
    config_dir : str
        Path to spandex configuration directory

    Returns
    -------
    None : None
        Loads shapefiles to the database (returns nothing)

    """
    def subpath(base_dir):
        def func(shp_table_name, shp_path):
            input_dir = base_dir
            return os.path.join(input_dir, shp_table_name, shp_path)
        return func
    for shape_category in shapefiles:
        path_func = subpath(os.path.join(data_dir,shape_category))
        shp_dict = shapefiles[shape_category]
        for shp_name in shp_dict:
            print 'Loading %s.' % shp_name
            path = path_func(shp_name, shp_dict[shp_name][0])
            load_shapefile(path, shape_category + '_' + shp_name, config_dir, shp_dict[shp_name][1])
